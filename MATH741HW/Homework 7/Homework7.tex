\documentclass{article}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage[margin=0.5in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ}
\usepackage{float}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{extarrows}
\graphicspath{./}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

% Define a new environment for problems
\newcounter{problemCounter}
\newtcolorbox{problem}[2][]{colback=white, colframe=black, boxrule=0.5mm, arc=4mm, auto outer arc, title={\ifstrempty{#1}{Problem \stepcounter{problemCounter}\theproblemCounter}{#1}}}

% \renewcommand{\labelenumi}{\alph{enumi})}
\def\zz{{\mathbb Z}}
\def\rr{{\mathbb R}}
\def\qq{{\mathbb Q}}
\def\cc{{\mathbb C}}
\def\nn{{\mathbb N}}
\def\ss{{\mathbb S}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtcolorbox{proposition}[1][]{colback=white, colframe=blue, boxrule=0.5mm, arc=4mm, auto outer arc, title={Proposition #1}}
\newtcolorbox{definition}[1][]{colback=white, colframe=violet, boxrule=0.5mm, arc=4mm, auto outer arc, title={Definition #1}}
\newcommand{\Zmod}[1]{\zz/#1\zz}
\newcommand{\partFrac}[2]{\frac{\partial #1}{\partial #2}}

\newcommand\Mydiv[2]{%
$\strut#1$\kern.25em\smash{\raise.3ex\hbox{$\big)$}}$\mkern-8mu
        \overline{\enspace\strut#2}$}

\begin{document}

\begin{center}
    Math 741
    \hfill Homework 7
    \hfill \textit{Stephen Cornelius}
\end{center}


% Start of Problems
\begin{problem} \\
    Let $V$ be an $n$-dimensional vector space, and let $\phi : V \to V$ be a linear map. Show that, for any $n$-linear antisymmetric form $\beta(v_1, \dots, v_n)$ on $n$, we have 
    \[
        \beta(\phi(v_1), \dots, \phi(v_n)) = \det(\phi) \beta(v_1, \dots, v_n).
    \]
    (This formalizes the following idea: any "unit of volume" on $V$, given by $\beta$, gets scaled by $\det(\phi)$ when we apply $\phi$. In fact, this can be used as a definition of $\det(\phi)$: this way, some of its properties, such as independence of basis and multiplicativity become clear.)
\end{problem}



\begin{proof}
    Let $\{ e_1, \dots, e_n \}$ be a basis of $V$. Then we can write 
    \[
        v_i = \sum_{j=1}^n a_{ij} e_j, \quad i = 1, \dots, n,
    \]
    for some scalars $a_{ij}$. Then we have 
    \begin{align*}
        \beta(\phi(v_1), \dots, \phi(v_n)) &= \beta\left( \phi\left( \sum_{j=1}^n a_{1j} e_j \right), \dots, \phi\left( \sum_{j=1}^n a_{nj} e_j \right) \right) \\
        &= \beta\left( \sum_{j=1}^n a_{1j} \phi(e_j), \dots, \sum_{j=1}^n a_{nj} \phi(e_j) \right) \\
        &= \sum_{j_1, j_2, \dots, j_n = 1}^n a_{1 j_1} a_{2 j_2} \cdots a_{n j_n} \beta(\phi(e_{j_1}), \phi(e_{j_2}), \dots, \phi(e_{j_n})) \\
        &= \sum_{\sigma \in S_n} a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)} \beta(\phi(e_{\sigma(1)}), \phi(e_{\sigma(2)}), \dots, \phi(e_{\sigma(n)})) \\
        &= \sum_{\sigma \in S_n} a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)} \operatorname{sgn}(\sigma) \beta(\phi(e_1), \phi(e_2), \dots, \phi(e_n)) \\
        &= \det(A) \beta(\phi(e_1), \phi(e_2), \dots, \phi(e_n)),
    \end{align*}
    where $A = (a_{ij})$ is the matrix whose columns are the coordinates of $v_i$ in the basis $\{ e_1, \dots, e_n \}$. Now, note that 
    \[
        \beta(\phi(e_1), \phi(e_2), \dots, \phi(e_n))
    \]
     is an $n$-linear antisymmetric form evaluated at the basis vectors $\{ e_1, \dots, e_n \}$ after applying $\phi$. By the definition of determinant, we have
    \[
        \beta(\phi(e_1), \phi(e_2), \dots, \phi(e_n)) = \det(\phi) \beta(e_1, e_2, \dots, e_n).
    \]
    Therefore, we conclude that
    \[
        \beta(\phi(v_1), \dots, \phi(v_n)) = \det(A) \det(\phi) \beta(e_1, e_2, \dots, e_n).
    \]
    Finally, since $\beta$ is multilinear, we have
    \[
        \beta(v_1, \dots, v_n) = \det(A) \beta(e_1, e_2, \dots, e_n).
    \]
    Combining these results, we obtain
    \[
        \beta(\phi(v_1), \dots, \phi(v_n)) = \det(\phi) \beta(v_1, \dots, v_n).
    \]
\end{proof}

\newpage
\begin{problem} \\
    Let $V$ be the space of polynomials of degree at most $n$ (over some field $K$; if you want, you can assume $K = \rr$). Fix $a,b \in \rr$ and consider the linear map
    \[
        \phi : V \to V, \quad p(t) \mapsto p(at + b).
    \]
    Compute $\det(\phi)$.
\end{problem}


\begin{proof}
    Let $\{ 1, t, t^2, \dots, t^n \}$ be the standard basis of $V$. Then we have 
    \[
        \phi(t^k) = (at + b)^k = \sum_{j=0}^k \binom{k}{j} a^j b^{k-j} t^j.
    \]
    Thus, the matrix representation of $\phi$ with respect to this basis is given by
    \[
        [\phi] = \begin{pmatrix}
            1 & b & b^2 & \cdots & b^n \\
            0 & a & 2ab & \cdots & n a b^{n-1} \\
            0 & 0 & a^2 & \cdots & \binom{n}{2} a^2 b^{n-2} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & a^n
        \end{pmatrix}.
    \]
    This is an upper triangular matrix, and the determinant of an upper triangular matrix is the product of its diagonal entries. Therefore, we have
    \[
        \det(\phi) = 1 \cdot a \cdot a^2 \cdot a^3 \cdots a^n = a^{\frac{n(n+1)}{2}}.
    \]
    Hence, the determinant of the linear map $\phi$ is 
    \[
        \det(\phi) = a^{\frac{n(n+1)}{2}}.
    \]
\end{proof}




\begin{problem} \\ 
    Let $M$ be an $n \times n$ matrix (over some field). Let $V$ be the space of $n \times m$ matrices. Consider the linear map $m_M : V \to V$ given by left multiplication by $M$:
    \[
        A \mapsto MA.
    \]
    Find $\det(m_M)$. (Of course, the answer depends on $M$.)
\end{problem}


\begin{proof}
    Let $\{ E_{ij} \mid 1 \leq i \leq n, 1 \leq j \leq m \}$ be the standard basis of $V$, where $E_{ij}$ is the matrix with a $1$ in the $(i,j)$-th position and $0$ elsewhere. Then we have 
    \[
        m_M(E_{ij}) = M E_{ij} = \text{the } j\text{-th column of } M \text{ placed in the } i\text{-th column of a zero matrix}.
    \]
    Thus, the matrix representation of $m_M$ with respect to this basis is given by a block matrix where each block corresponds to the action of $M$ on the respective basis element. Specifically, the matrix representation of $m_M$ can be viewed as an $nm \times nm$ matrix composed of $m$ blocks of size $n \times n$, each block being the matrix $M$. Therefore, the matrix representation of $m_M$ is given by
    \[
        [m_M] = \begin{pmatrix}
            M & 0 & 0 & \cdots & 0 \\
            0 & M & 0 & \cdots & 0 \\
            0 & 0 & M & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & M
        \end{pmatrix}.
    \]
    This is a block diagonal matrix with $m$ blocks of $M$. The determinant of a block diagonal matrix is the product of the determinants of its blocks. Therefore, we have
    \[
        \det(m_M) = (\det(M))^m.
    \]
    Hence, the determinant of the linear map $m_M$ is $\det(m_M) = (\det(M))^m$.
\end{proof}



\begin{problem} \\ 
    Let $K$ be a field and $V$ be a finite-dimensional vector space. Let 
    \[
        \gamma : V \times V \to K
    \]
    be an antisymmetric bilinear form. Show that there exists $k \leq \frac{n}{2}$ and a basis
    \[
        \{ a1, \dots, a_k, b_1, \dots, b_k, c_1, \dots, c_{n - 2k} \}
    \]
    of $V$ such that 
    \[
        \gamma(a_i, b_i) = 1, \quad \gamma(b_i, a_i) = -1, \quad (i = 1, \dots, k),
    \]
    and $\gamma$ vanishes on all other pairs of basis vectors. (When $n = 2k$, this is called a symplectic basis.)
\end{problem}

\begin{proof}
    We proceed by induction on the dimension $n$ of the vector space $V$. \\
    \textbf{Base Case:} If $n = 0$, the statement is trivially true with $k = 0$ and an empty basis. If $n = 1$, since $\gamma$ is antisymmetric, we have $\gamma(v, v) = 0$ for any $v \in V$. Thus, we can take $k = 0$ and any basis $\{ c_1 \}$ of $V$. \\
    \textbf{Inductive Step:} Assume the statement holds for all vector spaces of dimension less than $n$. We consider two cases: \\
    \begin{enumerate}[i.)]
        \item If $\gamma$ is the zero form, then we can take $k = 0$ and any basis $\{ c_1, \dots, c_n \}$ of $V$. \\
        \item If $\gamma$ is not the zero form, there exist vectors $u, v \in V$ such that $\gamma(u, v) \neq 0$. We can scale $u$ and $v$ such that $\gamma(u, v) = 1$. Since $\gamma(u,v) \neq 0$, we have that $u, v$ are linearly independent so by the building-up lemma we can extend $\{ u, v \}$ to a basis of $V$, say $\{ u, v, w_1, \dots, w_{n-2} \}$. Consider the subspace $W = \text{span}\{ w_1, \dots, w_{n-2} \}$. The restriction of $\gamma$ to $W$ is still an antisymmetric bilinear form. By the inductive hypothesis, there exists a basis of $W$ of the desired form with some $k' \leq \frac{n-2}{2}$. Adding $u$ and $v$ to this basis, we obtain a basis of $V$ of the desired form with $k = k' + 1 \leq \frac{n}{2}$. \\
    \end{enumerate}
    Thus, by induction, the statement holds for all finite-dimensional vector spaces $V$ over the field $K$.
\end{proof}



\begin{problem} \\ 
    Consider $\det(A)$ as a multivariable function of the entries of a real matrix $A$. Compute the directional derivative of this function at the point $A = I$ in the direction of some matrix $B$. (Equivalently, find the linear approximation for $f(t) = \det(I + tB)$ at $t = 0$.)
\end{problem}

\begin{proof}
    We want to compute the directional derivative of the determinant function at the identity matrix $I$ in the direction of a matrix $B$. The directional derivative can be expressed as
    \[
        D_B \det(I) = \lim_{t \to 0} \frac{\det(I + tB) - \det(I)}{t}.
    \]
    Since $\det(I) = 1$, we have
    \[
        D_B \det(I) = \lim_{t \to 0} \frac{\det(I + tB) - 1}{t}.
    \]
    To evaluate this limit, we can use the fact that for small $t$, the determinant can be approximated using the first-order term in its Taylor expansion. Specifically, we have
    \[
        \det(I + tB) = 1 + t \operatorname{tr}(B) + O(t^2),
    \]
    where $\operatorname{tr}(B)$ is the trace of the matrix $B$. Therefore, we have
    \[
        D_B \det(I) = \lim_{t \to 0} \frac{(1 + t \operatorname{tr}(B) + O(t^2)) - 1}{t} = \lim_{t \to 0} \frac{t \operatorname{tr}(B) + O(t^2)}{t}.
    \]
    Simplifying this expression, we get
    \[
        D_B \det(I) = \operatorname{tr}(B).
    \]
    Thus, the directional derivative of the determinant function at the identity matrix in the direction of the matrix $B$ is given by the trace of $B$:
    \[
        D_B \det(I) = \operatorname{tr}(B).
    \]
\end{proof}


\begin{problem} \\ 
    Let $A$ be a square $n \times n$ matrix whose characteristic polynomial has $n$ roots in $K$, counting with multiplicity. Consider the Jordan form of $A$: suppose that it consists of blocks $J_{\lambda_i, n_i}$, where $\lambda_i$ is the eigenvalue and $n_i$ is the size of the block. \\
    Express the following invariants of $A$ in terms of $n_i$ and $\lambda_i$: its characteristic polynomial, its minimal polynomial, the dimension of eigenspace for each $\lambda$ (this is called "the geometric multiplicity of an eigenvalue") and rank. (No explanation is required.)
\end{problem}

\begin{enumerate}[(a)]
    \item \textbf{Characteristic Polynomial:} The characteristic polynomial of $A$ is given by 
    \[
        p_A(x) = \prod_{i} (x - \lambda_i)^{n_i}.
    \]
    \item \textbf{Minimal Polynomial:} The minimal polynomial of $A$ is given by
    \[
        m_A(x) = \prod_{i} (x - \lambda_i)^{m_i},
    \]
    where $m_i$ is the size of the largest Jordan block corresponding to the eigenvalue $\lambda_i$.
    \item \textbf{Geometric Multiplicity:} The dimension of the eigenspace for each eigenvalue $\lambda_i$ is equal to the number of Jordan blocks associated with $\lambda_i$. If there are $k_i$ blocks for eigenvalue $\lambda_i$, then the geometric multiplicity is
    \[
        \text{geom. mult.}(\lambda_i) = k_i.
    \]
    \item \textbf{Rank:} The rank of the matrix $A$ can be computed as 
    \[
        \operatorname{rk}(A) = n - \sum_{i} (n_i - 1) = n - \left( \sum_{i} n_i - \text{number of blocks} \right).
    \]
\end{enumerate}




\begin{problem} \\ 
    Following up on the previous problem, let us go in the opposite direction: explain how to find $\lambda_i$ and $n_i$ from the data of $\operatorname{rk}(A - \lambda I)^k$ for all $\lambda \in K$ and $k > 0$. In particular, this implies that the Jordan form is unique.
\end{problem}

To find the eigenvalues $\lambda_i$ from the data of $\operatorname{rk}(A - \lambda I)^k$, we can use the fact that the eigenvalues of $A$ are precisely the values of $\lambda$ for which the matrix $A - \lambda I$ is not invertible. This occurs when $\det(A - \lambda I) = 0$. Thus, by examining the ranks of $(A - \lambda I)^k$ for various $\lambda$, we can identify the eigenvalues as those values of $\lambda$ for which the rank drops below $n$. \\
To determine the sizes of the Jordan blocks $n_i$, we can analyze the ranks of the powers of $(A - \lambda I)$. Specifically, for each eigenvalue $\lambda_i$, we consider the sequence of ranks:
\[
    \operatorname{rk}(A - \lambda_i I), \operatorname{rk}((A - \lambda_i I)^2), \operatorname{rk}((A - \lambda_i I)^3), \dots
\]
The size of the largest Jordan block corresponding to the eigenvalue $\lambda_i$ can be determined by finding the smallest integer $k$ such that
\[
    \operatorname{rk}((A - \lambda_i I)^k) = \operatorname{rk}((A - \lambda_i I)^{k+1}).
\]
This integer $k$ gives us the size of the largest Jordan block $n_i$ for the eigenvalue $\lambda_i$. By repeating this process for each eigenvalue, we can reconstruct the entire Jordan form of the matrix $A$. This method also shows that the Jordan form is unique, as the sizes of the Jordan blocks are determined solely by the ranks of the powers of $(A - \lambda I)$. \\
To find the other sizes of Jordan blocks corresponding to the same eigenvalue $\lambda_i$, we can use the differences in ranks:
\[
    \text{number of blocks of size } \geq k = \operatorname{rk}((A - \lambda_i I)^{k-1}) - \operatorname{rk}((A - \lambda_i I)^k).
\]


\end{document}